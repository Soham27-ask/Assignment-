{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9aae40f4",
      "metadata": {
        "id": "9aae40f4"
      },
      "source": [
        "## Question 1\n",
        "**Explain the differences between AI, ML, Deep Learning (DL), and Data Science (DS).**\n",
        "\n",
        "**Answer:**\n",
        "- **Artificial Intelligence (AI):** The broad field of building systems that can perform tasks that typically require human intelligence (reasoning, planning, perception, language, decision‑making). AI includes symbolic systems, search, rule‑based systems, ML, DL, etc.\n",
        "- **Machine Learning (ML):** A subfield of AI focused on algorithms that **learn patterns from data** to make predictions/decisions without being explicitly programmed. Examples: linear/logistic regression, decision trees, SVMs, clustering.\n",
        "- **Deep Learning (DL):** A subfield of ML that uses **multi‑layer neural networks** to learn complex representations (e.g., CNNs, RNNs, Transformers). Excels in perception and unstructured data (images, audio, text) at the cost of data/compute.\n",
        "- **Data Science (DS):** An end‑to‑end discipline that combines **statistics, ML, data engineering, and domain knowledge** to extract insights and support decisions. DS covers the whole lifecycle: problem framing, data collection, cleaning, analysis, modeling, evaluation, communication, and deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d17ef07",
      "metadata": {
        "id": "6d17ef07"
      },
      "source": [
        "## Question 2\n",
        "**What are the types of machine learning? Describe each with one real‑world example.**\n",
        "\n",
        "**Answer:**\n",
        "- **Supervised Learning:** Learn from labeled data (input → known target). *Example:* Fraud detection (transaction → fraud/not‑fraud).\n",
        "- **Unsupervised Learning:** Discover structure in unlabeled data. *Example:* Customer segmentation with clustering (K‑means).\n",
        "- **Semi‑Supervised Learning:** Train on small labeled + large unlabeled data. *Example:* Classifying product reviews when only some are labeled.\n",
        "- **Self‑Supervised Learning:** Create surrogate labels from data itself to pretrain models. *Example:* Masked‑word prediction for language models.\n",
        "- **Reinforcement Learning (RL):** Learn actions by interacting with an environment to maximize reward. *Example:* Recommendation systems optimizing long‑term engagement.\n",
        "- **Online/Incremental Learning:** Continuously update model with data stream. *Example:* Spam filters adapting to new spam patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b47e47a",
      "metadata": {
        "id": "7b47e47a"
      },
      "source": [
        "## Question 3\n",
        "**Define overfitting, underfitting, and the bias‑variance trade‑off.**\n",
        "\n",
        "**Answer:**\n",
        "- **Overfitting:** Model learns noise/idiosyncrasies; very low training error, high test error.\n",
        "- **Underfitting:** Model too simple; high error on both train and test.\n",
        "- **Bias‑Variance Trade‑off:** Increasing model complexity reduces **bias** (systematic error) but increases **variance** (sensitivity to data). Aim for a sweet spot (via regularization, more data, cross‑validation, early stopping, ensembling)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbc6886b",
      "metadata": {
        "id": "dbc6886b"
      },
      "source": [
        "## Question 4\n",
        "**What are outliers in a dataset, and list three common techniques for handling them.**\n",
        "\n",
        "**Answer:**\n",
        "Outliers are observations that **deviate markedly** from the rest of the data (genuine extremes or errors). Handling techniques:\n",
        "1. **Winsorizing/Capping** using IQR or percentile bounds.\n",
        "2. **Transformation** (e.g., log, Box‑Cox) to reduce skew.\n",
        "3. **Robust Modeling** or **robust scalers** (e.g., tree‑based models, Huber loss, RobustScaler).\n",
        "4. **Removal** if proven erroneous and justified (document carefully)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45434d6f",
      "metadata": {
        "id": "45434d6f"
      },
      "source": [
        "## Question 5\n",
        "**Explain handling missing values and mention one imputation technique for numerical and one for categorical data.**\n",
        "\n",
        "**Answer:**\n",
        "- **Process:** Inspect patterns (MCAR/MAR/MNAR), quantify missingness, decide per feature: drop rows/columns (if safe), or impute; encode missingness indicator if informative; validate effect via CV.\n",
        "- **Numerical imputation:** Mean/median imputation (e.g., `SimpleImputer(strategy='median')`).\n",
        "- **Categorical imputation:** Most‑frequent or a new category such as `'Missing'` (e.g., `SimpleImputer(strategy='most_frequent')` or `fill_value='Missing'`)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e01927bb",
      "metadata": {
        "id": "e01927bb"
      },
      "source": [
        "## Question 6\n",
        "**Create a synthetic imbalanced dataset with `make_classification()` and print the class distribution.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "347abcbd",
      "metadata": {
        "id": "347abcbd"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from collections import Counter\n",
        "\n",
        "X, y = make_classification(n_samples=2000, n_features=20, n_informative=3,\n",
        "                           n_redundant=2, n_repeated=0, n_clusters_per_class=1,\n",
        "                           weights=[0.95, 0.05], flip_y=0.01, random_state=42)\n",
        "\n",
        "print(\"Class distribution:\", Counter(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84008c68",
      "metadata": {
        "id": "84008c68"
      },
      "source": [
        "## Question 7\n",
        "**Implement one‑hot encoding using pandas for the list:** `['Red', 'Green', 'Blue', 'Green', 'Red']`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95704ab5",
      "metadata": {
        "id": "95704ab5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "colors = ['Red', 'Green', 'Blue', 'Green', 'Red']\n",
        "df = pd.DataFrame({'color': colors})\n",
        "encoded = pd.get_dummies(df, columns=['color'], prefix='', prefix_sep='')\n",
        "print(encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c72ca7ad",
      "metadata": {
        "id": "c72ca7ad"
      },
      "source": [
        "## Question 8\n",
        "**Generate 1000 samples from a normal distribution; introduce 50 random missing values; fill with the column mean; plot a histogram before and after imputation.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1210872",
      "metadata": {
        "id": "e1210872"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(0)\n",
        "data = np.random.normal(loc=0.0, scale=1.0, size=1000)\n",
        "df = pd.DataFrame({'x': data})\n",
        "\n",
        "# Introduce 50 missing values at random positions\n",
        "missing_idx = np.random.choice(df.index, size=50, replace=False)\n",
        "df.loc[missing_idx, 'x'] = np.nan\n",
        "\n",
        "# Histogram before imputation\n",
        "plt.figure()\n",
        "df['x'].hist(bins=30)\n",
        "plt.title('Histogram BEFORE imputation')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Impute with mean\n",
        "mean_val = df['x'].mean()\n",
        "df['x_filled'] = df['x'].fillna(mean_val)\n",
        "\n",
        "# Histogram after imputation\n",
        "plt.figure()\n",
        "df['x_filled'].hist(bins=30)\n",
        "plt.title('Histogram AFTER imputation (mean)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a869156",
      "metadata": {
        "id": "4a869156"
      },
      "source": [
        "## Question 9\n",
        "**Implement Min‑Max scaling on `[2, 5, 10, 15, 20]` using `MinMaxScaler`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5111827b",
      "metadata": {
        "id": "5111827b"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "arr = np.array([[2],[5],[10],[15],[20]], dtype=float)\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(arr)\n",
        "print(scaled.ravel())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a41a1bf1",
      "metadata": {
        "id": "a41a1bf1"
      },
      "source": [
        "## Question 10\n",
        "**Retail fraud dataset: data preparation plan (missing ages, outliers in amount, imbalanced target, categorical payment method).**\n",
        "\n",
        "**Step‑by‑step plan:**\n",
        "1. **Explore & audit:** Column types, missingness heatmap, basic stats, class imbalance, target leakage check.\n",
        "2. **Missing data (ages):** Impute with domain‑appropriate method (median for skewed ages) and optionally add a missing‑indicator.\n",
        "3. **Outliers (amount):** Cap using IQR/percentile bounds; alternatively apply a log transform; prefer **robust scalers**.\n",
        "4. **Imbalance (fraud vs non‑fraud):** Use appropriate metrics (ROC‑AUC, PR‑AUC), stratified CV; mitigate via **class weights**, **upsampling minority**, **downsampling majority**, or algorithmic methods (threshold tuning, focal loss models).\n",
        "5. **Categoricals (payment method):** One‑Hot Encode with `handle_unknown='ignore'`.\n",
        "6. **Pipelines:** Build a `ColumnTransformer` with imputation, encoding, and scaling; wrap in a supervised estimator; evaluate with stratified CV and a hold‑out set.\n",
        "7. **Monitoring:** Check drift, recalibrate thresholds, retrain schedule.\n",
        "\n",
        "**Illustrative code on a synthetic dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ab43218",
      "metadata": {
        "id": "8ab43218"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Synthetic data\n",
        "n = 5000\n",
        "ages = np.random.normal(35, 10, size=n).clip(18, 80)\n",
        "# introduce missing ages (10%)\n",
        "mask_missing = np.random.rand(n) < 0.1\n",
        "ages[mask_missing] = np.nan\n",
        "\n",
        "amount = np.random.lognormal(mean=3.2, sigma=0.7, size=n)\n",
        "# introduce outliers\n",
        "outlier_idx = np.random.choice(np.arange(n), size=15, replace=False)\n",
        "amount[outlier_idx] *= 10\n",
        "\n",
        "payment_methods = np.random.choice(['card', 'upi', 'netbanking', 'wallet'], size=n, p=[0.6, 0.25, 0.1, 0.05])\n",
        "\n",
        "# Imbalanced target: ~2% fraud\n",
        "y = (np.random.rand(n) < 0.02).astype(int)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'age': ages,\n",
        "    'amount': amount,\n",
        "    'payment_method': payment_methods,\n",
        "    'fraud': y\n",
        "})\n",
        "\n",
        "# ====== Simple outlier capping for `amount` via IQR ======\n",
        "Q1, Q3 = df['amount'].quantile([0.25, 0.75])\n",
        "IQR = Q3 - Q1\n",
        "lower = Q1 - 1.5 * IQR\n",
        "upper = Q3 + 1.5 * IQR\n",
        "df['amount_capped'] = df['amount'].clip(lower, upper)\n",
        "\n",
        "print(\"Class distribution BEFORE resampling:\", df['fraud'].value_counts().to_dict())\n",
        "\n",
        "# ====== Upsample minority class for training (simple illustration) ======\n",
        "df_major = df[df.fraud == 0]\n",
        "df_minor = df[df.fraud == 1]\n",
        "df_minor_up = resample(df_minor, replace=True, n_samples=len(df_major)//10, random_state=42)  # 1:10 ratio\n",
        "df_bal = pd.concat([df_major.sample(len(df_minor_up), random_state=42), df_minor_up], axis=0).sample(frac=1, random_state=42)\n",
        "\n",
        "print(\"Class distribution AFTER simple upsampling (train sample):\", df_bal['fraud'].value_counts().to_dict())\n",
        "\n",
        "X = df_bal[['age', 'amount_capped', 'payment_method']]\n",
        "y_bal = df_bal['fraud']\n",
        "\n",
        "num_features = ['age', 'amount_capped']\n",
        "cat_features = ['payment_method']\n",
        "\n",
        "numeric_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', RobustScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer([\n",
        "    ('num', numeric_pipeline, num_features),\n",
        "    ('cat', categorical_pipeline, cat_features)\n",
        "])\n",
        "\n",
        "clf = Pipeline(steps=[\n",
        "    ('prep', preprocess),\n",
        "    ('model', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_bal, test_size=0.2, stratify=y_bal, random_state=42)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "pred = clf.predict(X_test)\n",
        "proba = clf.predict_proba(X_test)[:,1]\n",
        "\n",
        "print(\"\\nClassification report (upsampled training):\\n\", classification_report(y_test, pred, digits=3))\n",
        "print(\"ROC-AUC:\", round(roc_auc_score(y_test, proba), 3))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}